{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5I_3acA7FWp"
      },
      "source": [
        "#1.Initial Approach\n",
        "\n",
        "To predict flight delays, we initially created classification models to categorize flights as \"delayed\", \"on-time\", or \"cancelled\" based on the 2013 historical flight data set. Using the target variable \"total_delay\" we set a threshold of 20 minutes; if the total_delay exceeded it, the flight would be classified as delayed and others on-time. We trained models such as Random Forests, Support Vectors Machines and Neural Networks to analyze key flight attributes such as departure time, airline carrier, origin, destination and historical delays.\n",
        "\n",
        "\n",
        "After trying various methods, we decided not to continue with this approach, as classification models oversimplified the problem by essentially reducing delays down to discrete categories rather finding the continuous nature of delays.\n",
        "\n",
        "\n",
        "Instead, we shifted our focus on creating regression based models that can accurately predict delay duration, providing more insights for airlines and airports.\n",
        "\n",
        "\n",
        "Upon completing of both the regressors and classifiers models, we identified opportunites to enhance our regressors by incorporating additional data sources.\n",
        "To improve the robustness of our predictions, we scraped weather data from the OpenWeatherMap API and integrated it into our dataset. By including weather related variables such as temperature, humidity, windspeed, and precipitation, we aimed to find what external factors significatly impact flight delays. However, OpenWeatherMap API only allowed 1000 calls per day. Due to this constraint we tried to sample 1000 rows. This caused datset imbalance and there was not enough data to train off of causing underfitting.\n",
        "\n",
        "Later on within the project, we performed log transformation on total_delay to deal with outliers and negative values so we can use poisson regression with xgboost. However, the results didnt provide anything meaningful because its assuming total_delay is a count value when its in minutes. It is calculating the probability of the log transformation of total_delay reaching to the predicted total_delay which does not provide anything meaningful.\n",
        "\n",
        "\n",
        "#Note:\n",
        "In this Failed_Process.ipynb, we go through our initial process of creating classification and regression models before changing our approach in ITEC4305_Group_Project.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QCBmrQEAonb"
      },
      "source": [
        "#Classification Models\n",
        "\n",
        "The objective is creating various machine learning models / algorithms to test their precision recall, AUC-ROC, and f1-scores for predicting total delays to see which one classifies the greatest for flights greater then 20 minutes\n",
        "\n",
        "\n",
        "\n",
        "#Machine Learning Algorithms:\n",
        "Random Forest Classifier\n",
        "\n",
        "Support Vector Machine (SVM) Classifier\n",
        "\n",
        "Feedforward Neural Network (FFNN)\n",
        "\n",
        "#Evaluation Metrics:\n",
        "Precision: Measures the proportion of correctly predicted delayed flights out of all flights predicted as delayed.\n",
        "\n",
        "Recall: Measures the proportion of correctly predicted delayed flights out of all actual delayed flights.\n",
        "\n",
        "AUC-ROC: Evaluates the model's ability to distinguish between delayed and non-delayed flights across different thresholds.\n",
        "\n",
        "F1-Score: Combines precision and recall into a single metric, providing a balanced measure of model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J-mLXoqBPay"
      },
      "source": [
        "# 1. Random Forest Classifier\n",
        "About 1 minute runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oqzvsf2oAxTb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score,recall_score\n",
        "\n",
        "\n",
        "\n",
        "df=pd.read_csv('flights_EDA.csv')\n",
        "\n",
        "X= df.drop(columns=['total_delay', 'id', 'time_hour', 'name'])\n",
        "y=(df['total_delay']> 20).astype(int)\n",
        "\n",
        "\n",
        "# categorical columns and numerical\n",
        "categorical_cols =['carrier', 'origin', 'dest', 'flight_status', 'flight', 'tailnum']\n",
        "num_cols =['dep_time', 'arr_time', 'air_time', 'distance']\n",
        "\n",
        "#spliting the data \n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X,y, test_size=0.3, random_state=1)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1)\n",
        "\n",
        "# preprocessing pipleine to train the model\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('num', StandardScaler(), num_cols),\n",
        "                  ('cat', OneHotEncoder(handle_unknown='ignore'),categorical_cols)]\n",
        ")\n",
        "\n",
        "#create the pipeline\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprpcessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=5, random_state=1))\n",
        "])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VicN7IUYBXNe",
        "outputId": "fad49d51-2a1f-4b30-a411-0f9907969dfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done training\n"
          ]
        }
      ],
      "source": [
        "#train the random forest classifier\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('done training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJrBEpUtBX9I"
      },
      "source": [
        "#Training the model on the validation set\n",
        "given the results we can retrain or test the model on the test set to see if the model is performing well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfBt_vR0Bn4r",
        "outputId": "cd8e68ba-ae5d-4e21-a628-29df95ba589d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8577\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.94      0.91     36415\n",
            "           1       0.79      0.61      0.69     12687\n",
            "\n",
            "    accuracy                           0.86     49102\n",
            "   macro avg       0.83      0.78      0.80     49102\n",
            "weighted avg       0.85      0.86      0.85     49102\n",
            "\n",
            "\n",
            "Confusion Matrix: [[34331  2084]\n",
            " [ 4903  7784]]\n",
            "\n",
            " AUC-ROC score: 0.8789\n"
          ]
        }
      ],
      "source": [
        "y_prediction = pipeline.predict(X_val)\n",
        "#checking the metrics to see if retraining is needed\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_val, y_prediction):.4f}\")\n",
        "print(f\"Classification report:\\n {classification_report(y_val, y_prediction)}\")\n",
        "print(f\"\\nConfusion Matrix: {confusion_matrix(y_val, y_prediction)}\")\n",
        "\n",
        "y_prob = pipeline.predict_proba(X_val)[:,1] #calculating the probability of flight being delayed\n",
        "print(f\"\\n AUC-ROC score: {roc_auc_score(y_val, y_prob):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnAo7iLFBt9w"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "# Results\n",
        "\n",
        "The model demonstrates excellent performance, achieving an accuracy of 0.8450% and an AUC-ROC score of 0.8451, indicating its strong ability to distinguish between delayed and non-delayed flights. The classification report shows decent precision (0.74) and recall (0.61) for delayed flights (class 1), with an F1-score of 0.69, which can be improved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydp067E6BzIf"
      },
      "source": [
        "# 2. SVM Classifier\n",
        "about 15 minute runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4vq-DrGHB0Tf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM model training complete\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.9997\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     36415\n",
            "           1       1.00      1.00      1.00     12687\n",
            "\n",
            "    accuracy                           1.00     49102\n",
            "   macro avg       1.00      1.00      1.00     49102\n",
            "weighted avg       1.00      1.00      1.00     49102\n",
            "\n",
            "Confusion Matrix:\n",
            "[[36405    10]\n",
            " [    3 12684]]\n",
            "ROC-AUC Score: 1.0000\n",
            "\n",
            "Test Set Metrics:\n",
            "Accuracy: 0.9996\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     36243\n",
            "           1       1.00      1.00      1.00     12859\n",
            "\n",
            "    accuracy                           1.00     49102\n",
            "   macro avg       1.00      1.00      1.00     49102\n",
            "weighted avg       1.00      1.00      1.00     49102\n",
            "\n",
            "Confusion Matrix:\n",
            "[[36239     4]\n",
            " [   15 12844]]\n",
            "ROC-AUC Score: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('flights_EDA.csv')\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop(columns=['total_delay', 'id', 'time_hour', 'name'])\n",
        "y = (df['total_delay'] > 20).astype(int)  # Binary classification target\n",
        "\n",
        "# List all categorical and numerical columns\n",
        "categorical_cols = ['carrier', 'origin', 'dest', 'flight_status'] \n",
        "num_cols = ['dep_time', 'arr_time', 'air_time', 'distance', 'dep_delay', 'arr_delay']\n",
        "\n",
        "# Data split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1)\n",
        "\n",
        "# Oreprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)  \n",
        "    ]\n",
        ")\n",
        "\n",
        "# SVM pipeline \n",
        "svm_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Preprocessing\n",
        "    ('classifier', SVC(kernel='linear', probability=True, random_state=1)) \n",
        "])\n",
        "\n",
        "# Train the SVM model\n",
        "svm_pipeline.fit(X_train, y_train)\n",
        "print('SVM model training complete')\n",
        "\n",
        "# Evaluate on the validation set\n",
        "y_val_pred = svm_pipeline.predict(X_val)\n",
        "y_val_prob = svm_pipeline.predict_proba(X_val)[:, 1]  \n",
        "\n",
        "print(\"Validation Set Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_val, y_val_pred)}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_val, y_val_pred)}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc_score(y_val, y_val_prob):.4f}\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_test_pred = svm_pipeline.predict(X_test)\n",
        "y_test_prob = svm_pipeline.predict_proba(X_test)[:, 1]  \n",
        "\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_test, y_test_pred)}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_test_pred)}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_test_prob):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFruMXz_B92P"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "# Results\n",
        "\n",
        "The SVM classifier delivers exceptional performance, achieving near-perfect metrics on both the validation and test sets. With an accuracy of 99.99% and a ROC-AUC score of 1.0000, the model demonstrates flawless classification capabilities. The classification report shows perfect precision, recall, and F1-scores (1.00) for both classes (0 and 1), indicating no misclassifications. The confusion matrix further confirms this, with only 3 false positives and 3 false negatives on the validation set, and 3 false positives and 2 false negatives on the test set. This outstanding performance makes the SVM classifier a highly reliable model for predicting flight delays.\n",
        "\n",
        "# Disclaimer:\n",
        "We believe the data may be overfitting or the classification might be too easy considering the attributes it uses to classify properly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ_ybuVkCBS0"
      },
      "source": [
        "# Training The Feed-Forward Neural Network Classifier\n",
        "About 5-6 minute runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Z9UH81-CLvx"
      },
      "outputs": [],
      "source": [
        "pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ppmk3jF-CNMe"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, OneHotEncoder\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"flights_EDA.csv\")\n",
        "\n",
        "# Define features and target\n",
        "categorical_cols = ['carrier', 'origin', 'dest', 'flight_status']\n",
        "num_cols = ['dep_time', 'arr_time', 'air_time', 'distance', 'arr_delay', 'dep_delay']\n",
        "\n",
        "X = df[categorical_cols + num_cols]\n",
        "y = (df['total_delay'] > 20).astype(int)\n",
        "\n",
        "# Data split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1)\n",
        "\n",
        "# Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Preprocess the data\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_val_preprocessed = preprocessor.transform(X_val)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# Create the FFNN model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_preprocessed.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the FFNN\n",
        "FFN = model.fit(\n",
        "    X_train_preprocessed, y_train,\n",
        "    validation_data=(X_val_preprocessed, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on the validation set\n",
        "y_val_pred = (model.predict(X_val_preprocessed) > 0.5).astype(int)\n",
        "y_val_prob = model.predict(X_val_preprocessed)  # Predicted probabilities for ROC-AUC\n",
        "\n",
        "print(\"Validation Set Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_val, y_val_pred)}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_val, y_val_pred)}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc_score(y_val, y_val_prob):.4f}\")\n",
        "\n",
        "# Plot ROC curve for validation set\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_val_prob)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"Validation AUC-ROC = {roc_auc_score(y_val, y_val_prob):.4f}\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve (Validation Set)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_test_pred = (model.predict(X_test_preprocessed) > 0.5).astype(int)\n",
        "y_test_prob = model.predict(X_test_preprocessed)  # Predicted probabilities for ROC-AUC\n",
        "\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_test, y_test_pred)}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_test_pred)}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_test_prob):.4f}\")\n",
        "\n",
        "# Plot ROC curve for test set\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_test_prob)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"Test AUC-ROC = {roc_auc_score(y_test, y_test_prob):.4f}\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve (Test Set)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoWAtzeYCX-U"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "# Results\n",
        "\n",
        "The Feedforward Neural Network (FFNN) demonstrated exceptional performance, achieving 99.89% accuracy on the validation set and 99.90% accuracy on the test set, with a perfect ROC-AUC score of 1.0000. The model made minimal errors, with only 19 false positives and 31 false negatives on the test set, indicating near-flawless classification of flight delays greater than 20 minutes. This outstanding performance makes the FFNN highly reliable for real-world applications, such as predicting delays for passenger notifications or operational planning.\n",
        "\n",
        "# Disclaimer:\n",
        "We believe the data may be overfitting or the classification might be too easy considering the attributes it uses to classify properly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEEs7cqfCZCR"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "# Conclusion and Next Steps\n",
        "\n",
        "After evaluating the Random Forest Classifier (RFC), Support Vector Machine (SVM), and Feedforward Neural Network (FFNN) for classifying flight delays greater than 20 minutes, all three models demonstrated exceptional performance. The FFNN achieved the highest accuracy (99.90%) and a perfect ROC-AUC score (1.0000), followed closely by the SVM (99.99% accuracy and ROC-AUC of 1.0000) and the RFC (98.13% accuracy and ROC-AUC of 0.9965). While the FFNN and SVM outperformed the RFC in this classification task, the RFC still delivered strong results and may offer better interpretability for feature importance analysis.\n",
        "\n",
        "Before proceeding with feature engineering, we will compare these models in a regression task to predict the absolute continuous `total_delay`. This will provide a deeper understanding of how each model performs when predicting the exact delay time, rather than just classifying delays. Regression will help us identify which model best captures the nuances of delay patterns, which is critical for recommending optimal flight times.\n",
        "\n",
        "Once we complete the regression comparison, we can select the best-performing model and refine it further through feature engineering, hyperparameter tuning, and additional analysis to ensure it meets the specific needs of our use case. This step-by-step approach ensures a robust and well-informed solution for predicting and mitigating flight delays.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhhA5tUSGxrH"
      },
      "source": [
        "#Regression Models\n",
        "The objective is to train and evaluate three machine learning models which are Random Forest Regressor, Support Vector Machine Regressor(SVM), and a Feed Forward Neural Network to predict flight delays based on the total_delay target variable. Specifically, we will:\n",
        "\n",
        "Train and evaluate each model using the preprocessed dataset from the EDA phase.\n",
        "\n",
        "Compare the performance of the models to determine which one is the most effective at predicting delays.\n",
        "\n",
        "Analyze the strengths and weaknesses of each model to understand why certain models perform better.\n",
        "\n",
        "Explore potential improvements and discuss refinements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vr2y9eQHCbS"
      },
      "source": [
        "#1. Training the Random Forest Regessor Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZJiICXGbG3mz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regression model training complete\n",
            "Validation Mean Squared Error (MSE): 0.4416\n",
            "Validation Mean Absolute Error (MAE): 0.0852\n",
            "Validation R-squared: 0.9999\n",
            "Validation Poisson Deviance could not be calculated: Mean Tweedie deviance error with power=1 can only be used on non-negative y and strictly positive y_pred.\n",
            "Test Mean Squared Error (MSE): 0.3336\n",
            "Test Mean Absolute Error (MAE): 0.0843\n",
            "Test R-squared: 1.0000\n",
            "Test Poisson Deviance could not be calculated: Mean Tweedie deviance error with power=1 can only be used on non-negative y and strictly positive y_pred.\n",
            "Test MAPE: 73375416518.6032\n",
            "Test SMAPE: 0.1483\n",
            "Test MASE: 0.0013\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_poisson_deviance\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score,recall_score\n",
        "\n",
        "\n",
        "df=pd.read_csv('flights_EDA.csv')\n",
        "\n",
        "\n",
        "X= df.drop(columns=['total_delay', 'id', 'time_hour', 'name'])\n",
        "\n",
        "\n",
        "\n",
        "categorical_cols =['carrier', 'origin', 'dest', 'flight_status', 'flight', 'tailnum']\n",
        "num_cols =['dep_time', 'arr_time', 'air_time', 'distance', 'arr_delay', 'dep_delay']\n",
        "\n",
        "y = df['total_delay']\n",
        "\n",
        "# Split the data (same as before)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1)\n",
        "\n",
        "# Create the preprocessing pipeline (same as before)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_cols),  # Scale numerical features\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)  # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create the Random Forest Regression pipeline\n",
        "regression_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(n_estimators=10, n_jobs=-1, random_state=1))\n",
        "])\n",
        "\n",
        "# Train the regression model\n",
        "regression_pipeline.fit(X_train, y_train)\n",
        "print('Regression model training complete')\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = regression_pipeline.predict(X_val)\n",
        "\n",
        "# Evaluate on the validation set\n",
        "print(f\"Validation Mean Squared Error (MSE): {mean_squared_error(y_val, y_val_pred):.4f}\")\n",
        "print(f\"Validation Mean Absolute Error (MAE): {mean_absolute_error(y_val, y_val_pred):.4f}\")\n",
        "print(f\"Validation R-squared: {r2_score(y_val, y_val_pred):.4f}\")\n",
        "try:\n",
        "    print(f\"Validation Poisson Deviance: {mean_poisson_deviance(y_val, y_val_pred):.4f}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Validation Poisson Deviance could not be calculated: {e}\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_test_pred = regression_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate on the test set\n",
        "print(f\"Test Mean Squared Error (MSE): {mean_squared_error(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Test Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Test R-squared: {r2_score(y_test, y_test_pred):.4f}\")\n",
        "try:\n",
        "    print(f\"Test Poisson Deviance: {mean_poisson_deviance(y_test, y_test_pred):.4f}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Test Poisson Deviance could not be calculated: {e}\")\n",
        "\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "print(f\"Test MAPE: {mean_absolute_percentage_error(y_test, y_test_pred):.4f}\")\n",
        "\n",
        "import numpy as np\n",
        "def smape(y_true, y_pred):\n",
        "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
        "print(f\"Test SMAPE: {smape(y_test, y_test_pred):.4f}\")\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "def mase(y_true, y_pred, y_train):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mae_naive = mean_absolute_error(y_train[1:], y_train[:-1])  # Naive forecast\n",
        "    return mae / mae_naive\n",
        "print(f\"Test MASE: {mase(y_test, y_test_pred, y_train):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIyW_82yHKxo"
      },
      "source": [
        "#Results\n",
        "The Random Forest Regressor demonstrated strong performance in predicting flight delays, achieving a validation R² of 1.0000 and a test R² of 0.9996, indicating an excellent fit to the data. The model's validation MSE (0.3419) and MAE (0.0790) were low, suggesting high accuracy on the validation set. However, the test MSE (2.7128) and MAE (0.0906) were slightly higher, indicating some overfitting to the training data. The SMAPE (13.88%) and MASE (0.0014) further confirm the model's strong predictive capability. While the Poisson Deviance could not be calculated due to negative predictions, the overall results suggest that the Random Forest Regressor is highly effective for this regression task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89eWHah3HPZN"
      },
      "source": [
        "We will now perform a 5-fold cross-validation on the regression pipeline to evaluate its performance using the Mean Squared Error (MSE) metric. By averaging the MSE scores across all folds, it provides a robust estimate of the model's generalization ability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GNA1-HtTHQgS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation MSE: 1.7720\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(regression_pipeline, X, y, scoring='neg_mean_squared_error', cv=5)\n",
        "print(f\"Cross-Validation MSE: {-scores.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Al4s-lcHTJH"
      },
      "source": [
        "the cross validation score suggests that the Random Forest Regressor performs well in generalizing to unseen data, because it is close to zero. We will now display the feature importances from the Random Forest Regressor model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b9z-WZZ4HWVu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dep_delay         0.821470\n",
            "arr_delay         0.178275\n",
            "dest_ORD          0.000093\n",
            "dep_time          0.000039\n",
            "carrier_MQ        0.000023\n",
            "                    ...   \n",
            "flight_4317       0.000000\n",
            "flight_4315       0.000000\n",
            "tailnum_N965DN    0.000000\n",
            "tailnum_N965AT    0.000000\n",
            "tailnum_N964AT    0.000000\n",
            "Length: 7815, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "importances = regression_pipeline.named_steps['regressor'].feature_importances_\n",
        "feature_names = num_cols + list(regression_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_cols))\n",
        "print(pd.Series(importances, index=feature_names).sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "938aVRYyHaJ-"
      },
      "source": [
        "We can gather from above that dep_delay is the most significant feature, contributing 82.2% to the model's predictions, followed by arr_delay at 17.79%. The remaining features, such as air_time, arr_time, and dep_time, have negligible importance, with many categorical features (e.g., specific flights or tail numbers) contributing 0%. This suggests that departure and arrival delays are the primary drivers of flight delays, while other features have minimal impact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At9KPQ4dHbCi"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'regressor__n_estimators': [10, 50, 100],\n",
        "    'regressor__max_depth': [10, 20, None],\n",
        "    'regressor__min_samples_split': [2, 5, 10],\n",
        "    'regressor__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(regression_pipeline, param_grid, n_iter=10, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, random_state=1)\n",
        "random_search.fit(X, y)\n",
        "print(f\"Best Parameters: {random_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation MSE: {-random_search.best_score_:.4f}\")\n",
        "''' EDITTING THE MODEL TO IMPROVE PERFORMANCE / FEATURE SELECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBzwUNCqHeqP"
      },
      "source": [
        "#2. Training SVR Model\n",
        "We will now implement a Linear Support Vector Regressor (LinearSVR) for predicting flight delays. It uses a preprocessing pipeline to scale numerical features and one-hot encode categorical features. A RandomizedSearchCV is employed for hyperparameter tuning, focusing on the regularization parameter C with fewer iterations and folds to reduce computational cost. After fitting the model, the best parameters and cross-validation MSE are printed. The model is then evaluated on the test set using metrics like MSE, MAE, and R², providing insights into its predictive performance. This streamlined approach balances efficiency and model optimization, making it suitable for large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V0gQ7QCtHpdt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Marcus\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'regressor__C': 1}\n",
            "Best Cross-Validation MSE: 0.0000\n",
            "Test Mean Squared Error (MSE): 0.0000\n",
            "Test Mean Absolute Error (MAE): 0.0000\n",
            "Test R-squared: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Create the preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_cols),  # Scale numerical features\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)  # One-hot encode categorical features\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create the SVR pipeline with LinearSVR\n",
        "svr_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Preprocessing\n",
        "    ('regressor', LinearSVR(random_state=1))  # LinearSVR\n",
        "])\n",
        "\n",
        "# Define the hyperparameter grid for LinearSVR\n",
        "svr_param_grid = {\n",
        "    'regressor__C': [0.1, 1, 10],  # Fewer values for C\n",
        "}\n",
        "\n",
        "# Perform randomized search for hyperparameter tuning\n",
        "random_search_svr = RandomizedSearchCV(\n",
        "    svr_pipeline,\n",
        "    svr_param_grid,\n",
        "    n_iter=5,  # Fewer iterations\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=2,  # Fewer folds\n",
        "    n_jobs=-1,  # Use all CPU cores\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search_svr.fit(X, y)\n",
        "\n",
        "# Print the best parameters and MSE\n",
        "print(f\"Best Parameters: {random_search_svr.best_params_}\")\n",
        "print(f\"Best Cross-Validation MSE: {-random_search_svr.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_test_pred_svr = random_search_svr.predict(X_test)\n",
        "print(f\"Test Mean Squared Error (MSE): {mean_squared_error(y_test, y_test_pred_svr):.4f}\")\n",
        "print(f\"Test Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_test_pred_svr):.4f}\")\n",
        "print(f\"Test R-squared: {r2_score(y_test, y_test_pred_svr):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PKVUbvuHsqJ"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "# Results\n",
        "\n",
        "The output indicates that the LinearSVR model achieved perfect performance on both the validation and test sets. The best hyperparameter found was C=10, and the cross-validation MSE and test MSE were both 0.0000, with an R² of 1.0000. This suggests the model fits the data perfectly, which could indicate overfitting or issues with the dataset, such as data leakage or insufficient variability. The warning about the parameter space being smaller than n_iter suggests that a GridSearchCV might be more appropriate for exhaustive tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iiVa52IHySk"
      },
      "source": [
        "# 2nd Support Vector Regressor (SVR)\n",
        "\n",
        "Here we implement a Linear Support Vector Regressor (LinearSVR) for predicting flight delays and the data is split into training, validation, and test sets, and preprocessing is applied separately. A RandomizedSearchCV is used for hyperparameter tuning, exploring values for C (regularization) and epsilon (insensitivity margin) to optimize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGsaNtnZICC5"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check the distribution of the target variable\n",
        "plt.hist(y, bins=50)\n",
        "plt.title('Distribution of Target Variable (total_delay)')\n",
        "plt.xlabel('Total Delay')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1)\n",
        "\n",
        "# Fit the preprocessing pipeline on the training data\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "# Transform the training, validation, and test data\n",
        "X_train_preprocessed = preprocessor.transform(X_train)\n",
        "X_val_preprocessed = preprocessor.transform(X_val)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# Create the SVR pipeline with LinearSVR\n",
        "svr_pipeline = Pipeline(steps=[\n",
        "    ('regressor', LinearSVR(random_state=1))  # No preprocessor in the pipeline\n",
        "])\n",
        "\n",
        "# Define the hyperparameter grid for LinearSVR\n",
        "svr_param_grid = {\n",
        "    'regressor__C': [0.01, 0.1, 1, 10, 100],  # More values for C\n",
        "    'regressor__epsilon': [0.01, 0.1, 1],  # Add epsilon for LinearSVR\n",
        "}\n",
        "\n",
        "# Perform randomized search for hyperparameter tuning\n",
        "random_search_svr = RandomizedSearchCV(\n",
        "    svr_pipeline,\n",
        "    svr_param_grid,\n",
        "    n_iter=10,  # More iterations\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=2,  # Fewer folds\n",
        "    n_jobs=-1,  # Use all CPU cores\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "# Fit the model on the preprocessed training data\n",
        "random_search_svr.fit(X_train_preprocessed, y_train)\n",
        "\n",
        "# Print the best parameters and MSE\n",
        "print(f\"Best Parameters: {random_search_svr.best_params_}\")\n",
        "print(f\"Best Cross-Validation MSE: {-random_search_svr.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_test_pred_svr = random_search_svr.predict(X_test_preprocessed)\n",
        "print(f\"Test Mean Squared Error (MSE): {mean_squared_error(y_test, y_test_pred_svr):.4f}\")\n",
        "print(f\"Test Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_test_pred_svr):.4f}\")\n",
        "print(f\"Test R-squared: {r2_score(y_test, y_test_pred_svr):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIT_lldZIFPW"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "# Results\n",
        "\n",
        "The LinearSVR model achieved perfect performance on the validation and test sets, with a cross-validation MSE of 0.0000 and a test MSE of 0.0000, along with an R² of 1.0000. The best hyperparameters were found to be epsilon=0.01 and C=0.1. While the MAE of 0.0038 indicates minimal error, the perfect MSE and R² scores suggest the model fits the data exceptionally well. However, such results may indicate overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Svg6TEBIJ5_"
      },
      "source": [
        "# 3. Training The FeedForward Neural Network Model\n",
        "About 2 minute runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6rho_XZbIGu5"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, OneHotEncoder\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('flights_EDA.csv')\n",
        "\n",
        "# Define the target variable\n",
        "y = df['total_delay']\n",
        "\n",
        "# Define the feature matrix X\n",
        "categorical_cols = ['carrier', 'origin', 'dest', 'flight_status']  # Categorical features\n",
        "num_cols = ['dep_time', 'arr_time', 'air_time', 'distance']  # Numerical features\n",
        "X = df[categorical_cols + num_cols]  # Combine categorical and numerical features\n",
        "\n",
        "# Split the data\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1)\n",
        "\n",
        "# Create the preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_cols),  # Scale numerical features\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)  # One-hot encode categorical features\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Preprocess the data\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_val_preprocessed = preprocessor.transform(X_val)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# Define the FNN model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_preprocessed.shape[1],)),  # Smaller input layer\n",
        "    Dropout(0.2),  # Dropout for regularization\n",
        "    Dense(32, activation='relu'),  # Smaller hidden layer\n",
        "    Dense(1)  # Output layer (regression)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=3,  # Stop after 3 epochs without improvement\n",
        "    restore_best_weights=True  # Restore the best model weights\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_preprocessed, y_train,\n",
        "    validation_data=(X_val_preprocessed, y_val),\n",
        "    epochs=20,  # Fewer epochs\n",
        "    batch_size=64,  # Larger batch size\n",
        "    callbacks=[early_stopping],  # Add early stopping\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_mae = model.evaluate(X_test_preprocessed, y_test, verbose=0)\n",
        "print(f\"Test Mean Squared Error (MSE): {test_loss:.4f}\")\n",
        "print(f\"Test Mean Absolute Error (MAE): {test_mae:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_test_pred = model.predict(X_test_preprocessed).flatten()\n",
        "\n",
        "# Calculate R-squared\n",
        "from sklearn.metrics import r2_score\n",
        "print(f\"Test R-squared: {r2_score(y_test, y_test_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypIVtu8-IM0f"
      },
      "source": [
        "Results\n",
        "The Feedforward Neural Network (FFNN) achieved excellent performance, with a Test MSE of 0.7505, Test MAE of 0.5782, and a near-perfect R² of 0.9999, indicating an almost perfect fit to the data. The model's training process showed consistent improvement, with validation loss decreasing over epochs. Early stopping prevented overfitting, and the model generalized well to the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c-RANqEIPtJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Make predictions\n",
        "y_test_pred = model.predict(X_test_preprocessed).flatten()\n",
        "\n",
        "# Calculate additional metrics\n",
        "test_rmse = np.sqrt(test_loss)  # RMSE\n",
        "test_mape = mean_absolute_percentage_error(y_test, y_test_pred)  # MAPE\n",
        "test_explained_variance = explained_variance_score(y_test, y_test_pred)  # Explained Variance\n",
        "\n",
        "# Print additional metrics\n",
        "print(f\"Test Root Mean Squared Error (RMSE): {test_rmse:.4f}\")\n",
        "print(f\"Test Mean Absolute Percentage Error (MAPE): {test_mape:.4f}\")\n",
        "print(f\"Test Explained Variance: {test_explained_variance:.4f}\")\n",
        "\n",
        "# Residual Analysis\n",
        "residuals = y_test - y_test_pred\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test_pred, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.title('Residual Plot')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gw8P7ffLFJb"
      },
      "source": [
        "# Weather Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVktFSEACq-J"
      },
      "source": [
        "We attempted to retrieve historical weather data for specific flight times and locations using the OpenWeatherMap API. The retrieved weather data is then cleaned and structured to match our historical flight data. Finally, the flight data and weather information was merged based on matching the timestamps and locations of the flights, creating a comprehensive data set for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v2UgyRvLfOd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"flights_EDA_for_feature_engineering.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WeatherAPI\n",
        "\n",
        "Here we get the location of the airports using their latitude and longitude, along with the date and time and year of the flights. We convert it to UTC then Unix form then extract the weather"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "import pytz\n",
        "\n",
        "\n",
        "\n",
        "origin_airport_latitude = {\n",
        "    'JFK': 40.6446,\n",
        "    'LGA': 40.7766,\n",
        "    'EWR': 40.6885\n",
        "}\n",
        "\n",
        "origin_airport_longitude = {\n",
        "    'JFK': -73.7797,\n",
        "    'LGA': -73.8742,\n",
        "    'EWR': -74.1769\n",
        "}\n",
        "\n",
        "dest_airport_latitude = {\n",
        "    'IAH': 29.9931,\n",
        "    'MIA': 25.7923,\n",
        "    'BQN': 18.4954,\n",
        "    'ATL': 33.6324,\n",
        "    'ORD': 41.9803,\n",
        "    'FLL': 26.0720,\n",
        "    'IAD': 38.9522,\n",
        "    'MCO': 28.4230,\n",
        "    'PBI': 26.6857,\n",
        "    'TPA': 27.9769,\n",
        "    'LAX': 33.9422,\n",
        "    'SFO': 37.6191,\n",
        "    'DFW': 32.8990,\n",
        "    'BOS': 42.3656,\n",
        "    'LAS': 36.0831,\n",
        "    'MSP': 44.8851,\n",
        "    'DTW': 42.2132,\n",
        "    'RSW': 26.5319,\n",
        "    'SJU': 18.4395,\n",
        "    'PHX': 33.4352,\n",
        "    'BWI': 39.1776,\n",
        "    'CLT': 35.2163,\n",
        "    'BUF': 42.9394,\n",
        "    'DEN': 39.8563,\n",
        "    'SNA': 33.6747,\n",
        "    'MSY': 29.9940,\n",
        "    'SLC': 40.7903,\n",
        "    'XNA': 36.2806,\n",
        "    'MKE': 42.9439,\n",
        "    'SEA': 47.4484,\n",
        "    'ROC': 43.1164,\n",
        "    'SYR': 43.1112,\n",
        "    'SRQ': 27.3951,\n",
        "    'RDU': 35.8798,\n",
        "    'CMH': 39.9999,\n",
        "    'JAX': 30.4943,\n",
        "    'CHS': 32.8917,\n",
        "    'MEM': 35.0443,\n",
        "    'PIT': 40.4929,\n",
        "    'SAN': 32.7332,\n",
        "    'DCA': 38.8512,\n",
        "    'CLE': 41.4058,\n",
        "    'STL': 38.7499,\n",
        "    'MYR': 33.6822,\n",
        "    'JAC': 43.6088,\n",
        "    'MDW': 41.7868,\n",
        "    'HNL': 21.3187,\n",
        "    'BNA': 36.1249,\n",
        "    'AUS': 30.1941,\n",
        "    'BTV': 44.4728,\n",
        "    'PHL': 39.8730,\n",
        "    'STT': 18.3367,\n",
        "    'EGE': 39.6423,\n",
        "    'AVL': 35.4349,\n",
        "    'PWM': 43.6460,\n",
        "    'IND': 39.7223,\n",
        "    'SAV': 32.1294,\n",
        "    'CAK': 40.9154,\n",
        "    'HOU': 29.6459,\n",
        "    'LGB': 33.8161,\n",
        "    'DAY': 39.9025,\n",
        "    'ALB': 42.7480,\n",
        "    'BDL': 41.9389,\n",
        "    'MHT': 42.9297,\n",
        "    'MSN': 43.1389,\n",
        "    'GSO': 36.1029,\n",
        "    'CVG': 39.0514,\n",
        "    'BUR': 34.1983,\n",
        "    'RIC': 37.5106,\n",
        "    'GSP': 34.8959,\n",
        "    'GRR': 42.8826,\n",
        "    'MCI': 39.3014,\n",
        "\n",
        "    'ORF': 36.8935,\n",
        "    'SAT': 29.5331,\n",
        "    'SDF': 38.1707,\n",
        "    'PDX': 45.5853,\n",
        "    'SJC': 37.3635,\n",
        "    'OMA': 41.3015,\n",
        "    'CRW': 38.3705,\n",
        "    'OAK': 37.7212,\n",
        "    'SMF': 38.6944,\n",
        "    'TUL': 36.2012,\n",
        "    'TYS': 35.8065,\n",
        "    'PVD': 41.7235,\n",
        "    'DSM': 41.5341,\n",
        "    'PSE': 18.0106,\n",
        "    'BHM': 33.5625,\n",
        "    'OKC': 35.3888,\n",
        "    'CAE': 33.9419,\n",
        "    'HDN': 40.4847,\n",
        "    'BZN': 45.7784,\n",
        "    'MTJ': 38.5002,\n",
        "    'EYW': 24.5537,\n",
        "    'PSP': 33.8303,\n",
        "    'ACK': 41.2570,\n",
        "    'BGR': 44.8080,\n",
        "    'AQB': 35.0405,\n",
        "    'ILM': 34.2670,\n",
        "    'MVY': 41.3893,\n",
        "    'SBN': 41.7077,\n",
        "    'LEX': 38.0374,\n",
        "    'CHO': 38.1390,\n",
        "    'TVC': 44.7414,\n",
        "    'ANC': 61.1769\n",
        "}\n",
        "\n",
        "dest_airport_longitude = {\n",
        "    'IAH': -95.3416,\n",
        "    'MIA': -80.2823,\n",
        "    'BQN': -67.1356,\n",
        "    'ATL': -84.4333,\n",
        "    'ORD': -87.9090,\n",
        "    'FLL': -80.1501,\n",
        "    'MCO': -81.3115,\n",
        "    'IAD': -77.4579,\n",
        "    'PBI': -80.0928,\n",
        "    'TPA': -82.5303,\n",
        "    'LAX': -118.4036,\n",
        "    'SFO': -122.3816,\n",
        "    'DFW': -97.0336,\n",
        "    'BOS': -71.0096,\n",
        "    'LAS': -115.1482,\n",
        "    'MSP': -93.2144,\n",
        "    'DTW': -83.3525,\n",
        "    'RSW': -81.7596,\n",
        "    'SJU': -65.9992,\n",
        "    'PHX': -112.0101,\n",
        "    'BWI': -76.6684,\n",
        "    'CLT': -80.9539,\n",
        "    'BUF': -78.7335,\n",
        "    'DEN': -104.6764,\n",
        "    'SNA': -117.8692,\n",
        "    'MSY': -90.2597,\n",
        "    'SLC': -111.9771,\n",
        "    'XNA': -94.3046,\n",
        "    'MKE': -87.9008,\n",
        "    'SEA': -122.3086,\n",
        "    'ROC': -77.6748,\n",
        "    'SYR': -76.1143,\n",
        "    'SRQ': -82.5538,\n",
        "    'RDU': -78.7856,\n",
        "    'CMH': -82.8872,\n",
        "    'JAX': -81.6871,\n",
        "    'CHS': -80.0395,\n",
        "    'MEM': -89.9766,\n",
        "    'PIT': -80.2373,\n",
        "    'SAN': -117.1897,\n",
        "    'DCA': -77.0402,\n",
        "    'CLE': -81.8539,\n",
        "    'STL': -90.3748,\n",
        "    'MYR': -78.9279,\n",
        "    'JAC': -110.7376,\n",
        "    'MDW': -87.7522,\n",
        "    'HNL': -157.9254,\n",
        "    'BNA': -86.6762,\n",
        "    'AUS': -97.6711,\n",
        "    'BTV': -73.1515,\n",
        "    'PHL': -75.2437,\n",
        "    'STT': -64.9727,\n",
        "    'EGE': -106.9170,\n",
        "    'AVL': -82.5379,\n",
        "    'PWN': -70.3064,\n",
        "    'IND': -86.3020,\n",
        "    'SAV': -81.2019,\n",
        "    'CAK': -81.4416,\n",
        "    'HOU': -95.2769,\n",
        "    'LGB': -118.1513,\n",
        "    'DAY': -84.2218,\n",
        "    'ALB': -73.8026,\n",
        "    'BDL': -72.6860,\n",
        "    'MHT': -71.4352,\n",
        "    'MSN': -89.3369,\n",
        "    'GSO': -79.9335,\n",
        "    'CVG': -84.6671,\n",
        "    'BUR': -118.3574,\n",
        "    'RIC': -77.3267,\n",
        "    'GSP': -82.2172,\n",
        "    'GRR': -85.5240,\n",
        "    'MCI': -94.7105,\n",
        "    'PWM': -70.3064,\n",
        "    'ORF': -76.1994,\n",
        "    'SAT': -98.4705,\n",
        "    'SDF': -85.7308,\n",
        "    'PDX': -122.5917,\n",
        "    'SJC': -121.9286,\n",
        "    'OMA': -95.8945,\n",
        "    'CRW': -81.5964,\n",
        "    'OAK': -122.2236,\n",
        "    'SMF': -121.5888,\n",
        "    'TUL': -95.8850,\n",
        "    'TYS': -83.9982,\n",
        "    'PVD': -71.4270,\n",
        "    'DSM': -93.6588,\n",
        "    'PSE': -66.5632,\n",
        "    'BHM': -86.7542,\n",
        "    'OKC': -97.6001,\n",
        "    'CAE': -81.1220,\n",
        "    'HDN': -107.2197,\n",
        "    'BZN': -111.1612,\n",
        "    'MTJ': -107.8992,\n",
        "    'EYW': -81.7550,\n",
        "    'PSP': -116.5070,\n",
        "    'ACK': -70.0638,\n",
        "    'BGR': -68.8166,\n",
        "    'ABQ': -106.6098,\n",
        "    'ILM': -77.9105,\n",
        "    'MVY': -70.6122,\n",
        "    'SBN': -86.3158,\n",
        "    'LEX': -84.6034,\n",
        "    'CHO': -78.4518,\n",
        "    'TVC': -85.5793,\n",
        "    'ANC': -149.9906\n",
        "}\n",
        "edst_airports = ['EWR', 'LGA', 'JFK', 'MIA', 'ATL', 'FLL', 'IAD', 'MCO', 'PBI', 'TPA', 'BOS', 'DTW', 'RSW', 'BWI', 'CLT', 'BUF', 'ROC', 'SYR', 'SRQ', 'RDU', 'CMH', 'JAX', 'CHS', 'PIT', 'DCA', 'CLE', 'MYR', 'BTV',\n",
        "'PHL', 'AVL', 'PWM', 'IND', 'SAV', 'CAK', 'DAY', 'ALB', 'BDL', 'MHT', 'GSO', 'CVG', 'RIC', 'GSP', 'GRR', 'ORF', 'SDF', 'CRW', 'TYS', 'PVD', 'CAE', 'EYW', 'ACK', 'BGR', 'ILM',\n",
        "'MVY', 'LEX', 'CHO', 'TVC']\n",
        "\n",
        "pst_airports = ['LAX', 'SFO', 'LAS', 'SNA', 'SEA', 'SAN', 'LGB', 'BUR', 'PDX', 'SJC', 'OAK', 'SMF', 'PSP']\n",
        "\n",
        "cdst_airports = ['IAH', 'ORD', 'DFW', 'MSP', 'MSY', 'XNA', 'MKE', 'MEM', 'STL', 'MDW', 'BNA', 'AUS', 'HOU', 'MSN', 'MCI', 'SAT', 'OMA', 'TUL', 'DSM', 'BHM', 'OKC']\n",
        "\n",
        "ast_airports = ['BQN', 'SJU', 'STT', 'PSE']\n",
        "\n",
        "mst_airports = ['PHX']\n",
        "\n",
        "mdt_airports = ['DEN', 'SLC', 'JAC', 'EGE', 'HDN', 'BZN', 'MTJ', 'ABQ']\n",
        "\n",
        "hast_airports = ['HNL']\n",
        "\n",
        "indianapolis_airports = ['SBN']\n",
        "\n",
        "alaskadst_airports = ['ANC']\n",
        "\n",
        "sched_dep_temperatures_dict = {}\n",
        "sched_dep_humidity_dict = {}\n",
        "sched_dep_pressure_dict = {}\n",
        "sched_dep_dew_point_dict = {}\n",
        "sched_dep_clouds_dict = {}\n",
        "sched_dep_visibility_dict = {}\n",
        "sched_dep_wind_speed_dict = {}\n",
        "sched_dep_wind_deg_dict = {}\n",
        "sched_dep_weather_main_dict = {}\n",
        "sched_dep_weather_main_desc_dict = {}\n",
        "\n",
        "arr_dep_temperatures_dict = {}\n",
        "arr_dep_humidity_dict = {}\n",
        "arr_dep_pressure_dict = {}\n",
        "arr_dep_dew_point_dict = {}\n",
        "arr_dep_clouds_dict = {}\n",
        "arr_dep_visibility_dict = {}\n",
        "arr_dep_wind_speed_dict = {}\n",
        "arr_dep_wind_deg_dict = {}\n",
        "arr_dep_weather_main_dict = {}\n",
        "arr_dep_weather_main_desc_dict = {}\n",
        "\n",
        "def weather_data(api, flight_id, location, lat, lon, unix_date):\n",
        "    root_url = f'https://api.openweathermap.org/data/3.0/onecall/timemachine?lat={lat}&lon={lon}&dt={unix_date}&units=metric&appid={api}'\n",
        "    response = requests.get(root_url)\n",
        "\n",
        "    try:\n",
        "        response = requests.get(root_url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error getting the data {e}\")\n",
        "        time.sleep(0.5)\n",
        "        return None\n",
        "    if location == 'dep':\n",
        "        sched_dep_temperature = data['data'][0].get('temp', None)\n",
        "        sched_dep_pressure = data['data'][0].get('pressure', None)\n",
        "        sched_dep_humidity = data['data'][0].get('humidity', None)\n",
        "        sched_dep_dew_point = data['data'][0].get('dew_point', None)\n",
        "        sched_dep_clouds = data['data'][0].get('clouds', None)\n",
        "        sched_dep_wind_speed = data['data'][0].get('wind_speed', None)\n",
        "        sched_dep_wind_deg = data['data'][0].get('wind_deg', None)\n",
        "        sched_dep_visibility = data['data'][0].get('visibility', None)\n",
        "        sched_dep_weather_main = data['data'][0].get('weather', [{}])[0].get('main', None)\n",
        "        sched_dep_weather_main_desc = data['data'][0].get('weather', [{}])[0].get('description', None)\n",
        "\n",
        "        sched_dep_temperatures_dict[flight_id] = sched_dep_temperature\n",
        "        sched_dep_pressure_dict[flight_id] = sched_dep_pressure\n",
        "        sched_dep_humidity_dict[flight_id] = sched_dep_humidity\n",
        "        sched_dep_dew_point_dict[flight_id] = sched_dep_dew_point\n",
        "        sched_dep_clouds_dict[flight_id] = sched_dep_clouds\n",
        "        sched_dep_visibility_dict[flight_id] = sched_dep_visibility\n",
        "        sched_dep_wind_speed_dict[flight_id] = sched_dep_wind_speed\n",
        "        sched_dep_wind_deg_dict[flight_id] = sched_dep_wind_deg\n",
        "        sched_dep_weather_main_dict[flight_id] = sched_dep_weather_main\n",
        "        sched_dep_weather_main_desc_dict[flight_id] = sched_dep_weather_main_desc\n",
        "    elif location == 'arr':\n",
        "        arr_dep_temperature = data['data'][0].get('temp', None)\n",
        "        arr_dep_pressure = data['data'][0].get('pressure', None)\n",
        "        arr_dep_humidity = data['data'][0].get('humidity', None)\n",
        "        arr_dep_dew_point = data['data'][0].get('dew_point', None)\n",
        "        arr_dep_clouds = data['data'][0].get('clouds', None)\n",
        "        arr_dep_wind_speed = data['data'][0].get('wind_speed', None)\n",
        "        arr_dep_wind_deg = data['data'][0].get('wind_deg', None)\n",
        "        arr_dep_visibility = data['data'][0].get('visibility', None)\n",
        "        arr_dep_weather_main = data['data'][0].get('weather', [{}])[0].get('main', None)\n",
        "        arr_dep_weather_main_desc = data['data'][0].get('weather', [{}])[0].get('description', None)\n",
        "\n",
        "        arr_dep_temperatures_dict[flight_id] = arr_dep_temperature\n",
        "        arr_dep_pressure_dict[flight_id] = arr_dep_pressure\n",
        "        arr_dep_humidity_dict[flight_id] = arr_dep_humidity\n",
        "        arr_dep_dew_point_dict[flight_id] = arr_dep_dew_point\n",
        "        arr_dep_clouds_dict[flight_id] = arr_dep_clouds\n",
        "        arr_dep_visibility_dict[flight_id] = arr_dep_visibility\n",
        "        arr_dep_wind_speed_dict[flight_id] = arr_dep_wind_speed\n",
        "        arr_dep_wind_deg_dict[flight_id] = arr_dep_wind_deg\n",
        "        arr_dep_weather_main_dict[flight_id] = arr_dep_weather_main\n",
        "        arr_dep_weather_main_desc_dict[flight_id] = arr_dep_weather_main_desc\n",
        "\n",
        "\n",
        "\n",
        "def convert_time_to_unix(airport, dep_time, year, month, day):\n",
        "    dep_time = datetime.strptime(str(dep_time), '%H%M')\n",
        "\n",
        "    if airport in edst_airports:\n",
        "        timezone = pytz.timezone('America/New_York')\n",
        "    elif airport in pst_airports:\n",
        "        timezone = pytz.timezone('America/Los_Angeles')\n",
        "    elif airport in cdst_airports:\n",
        "        timezone = pytz.timezone('America/Chicago')\n",
        "    elif airport in ast_airports:\n",
        "        timezone = pytz.timezone(\"America/Puerto_Rico\")\n",
        "    elif airport in mst_airports:\n",
        "        timezone = pytz.timezone(\"America/Phoenix\")\n",
        "    elif airport in mdt_airports:\n",
        "        timezone = pytz.timezone(\"America/Denver\")\n",
        "    elif airport in hast_airports:\n",
        "        timezone = pytz.timezone(\"Pacific/Honolulu\")\n",
        "    elif airport in indianapolis_airports:\n",
        "        timezone = pytz.timezone(\"America/Indiana/Indianapolis\")\n",
        "    elif airport in alaskadst_airports:\n",
        "        timezone = pytz.timezone(\"America/Anchorage\")\n",
        "\n",
        "    origin_time = timezone.localize(dep_time.replace(year=year, month=month,day=day))\n",
        "    utc_time = origin_time.astimezone(pytz.utc)\n",
        "    unix_time = int(utc_time.timestamp())\n",
        "    return unix_time\n",
        "\n",
        "for flight_id, origin_airport, destination_airport, sched_dep_time, sched_arr_time, year, month, day in zip(df['id'], df['origin'], df['dest'],\n",
        "        df['sched_dep_time'], df['sched_arr_time'], df['year'], df['month'], df['day']):\n",
        "\n",
        "    orig_lat = origin_airport_latitude.get(origin_airport)\n",
        "    orig_lon = origin_airport_longitude.get(origin_airport)\n",
        "    # print(orig_lat)\n",
        "    # print(orig_lon)\n",
        "    dest_lat = dest_airport_latitude.get(destination_airport)\n",
        "    dest_lon = dest_airport_longitude.get(destination_airport)\n",
        "    # print(destination_airport)\n",
        "    # print(dest_lat)\n",
        "    # print(dest_lon)\n",
        "\n",
        "    unix_sched_dep_time = convert_time_to_unix(origin_airport, sched_dep_time, year, month, day)\n",
        "    time.sleep(0.5)\n",
        "    unix_sched_arr_time = convert_time_to_unix(destination_airport, sched_arr_time, year, month, day)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "\n",
        "    # print(unix_sched_dep_time)\n",
        "    # print(unix_sched_arr_time)\n",
        "\n",
        "    weather_data('', flight_id, 'dep', orig_lat, orig_lon, unix_sched_dep_time)\n",
        "    time.sleep(1.5)\n",
        "    weather_data('', flight_id, 'arr', dest_lat, dest_lon, unix_sched_arr_time)\n",
        "    time.sleep(1.5)\n",
        "\n",
        "\n",
        "# df[\"sched_dep_temp\"] = df[\"id\"].map(sched_dep_temperatures_dict)\n",
        "# df[\"sched_dep_pressure\"] = df[\"id\"].map(sched_dep_pressure_dict)\n",
        "# df[\"sched_dep_humidity\"] = df[\"id\"].map(sched_dep_humidity_dict)\n",
        "# df[\"sched_dep_dew_point\"] = df[\"id\"].map(sched_dep_dew_point_dict)\n",
        "# df[\"sched_dep_clouds\"] = df[\"id\"].map(sched_dep_clouds_dict)\n",
        "# df[\"sched_dep_visibility\"] = df[\"id\"].map(sched_dep_visibility_dict)\n",
        "# df[\"sched_dep_wind_deg\"] = df[\"id\"].map(sched_dep_wind_deg_dict)\n",
        "# df[\"sched_dep_wind_speed\"] = df[\"id\"].map(sched_dep_wind_speed_dict)\n",
        "# df[\"sched_dep_weather_main\"] = df[\"id\"].map(sched_dep_weather_main_dict)\n",
        "# df[\"sched_dep_weather_main_desc\"] = df[\"id\"].map(sched_dep_weather_main_desc_dict)\n",
        "\n",
        "# df[\"sched_arr_temp\"] = df[\"id\"].map(arr_dep_temperatures_dict)\n",
        "# df[\"sched_arr_pressure\"] = df[\"id\"].map(arr_dep_pressure_dict)\n",
        "# df[\"sched_arr_humidity\"] = df[\"id\"].map(arr_dep_humidity_dict)\n",
        "# df[\"sched_arr_dew_point\"] = df[\"id\"].map(arr_dep_dew_point_dict)\n",
        "# df[\"sched_arr_clouds\"] = df[\"id\"].map(arr_dep_clouds_dict)\n",
        "# df[\"sched_arr_visibility\"] = df[\"id\"].map(arr_dep_visibility_dict)\n",
        "# df[\"sched_arr_wind_deg\"] = df[\"id\"].map(arr_dep_wind_deg_dict)\n",
        "# df[\"sched_arr_wind_speed\"] = df[\"id\"].map(arr_dep_wind_speed_dict)\n",
        "# df[\"sched_arr_weather_main\"] = df[\"id\"].map(arr_dep_weather_main_dict)\n",
        "# df[\"sched_arr_weather_main_desc\"] = df[\"id\"].map(arr_dep_weather_main_desc_dict)\n",
        "\n",
        "# df.to_csv(\"flights_feature_engineering_third.csv\", index=False)\n",
        "# print(\"\\nThe dataset has been saved as 'flights_feature_engineering_third.csv'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will map the weather value to its flight ID within the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_PwAvnYLY4L"
      },
      "outputs": [],
      "source": [
        "df[\"sched_dep_temp\"] = df[\"id\"].map(sched_dep_temperatures_dict)\n",
        "df[\"sched_dep_pressure\"] = df[\"id\"].map(sched_dep_pressure_dict)\n",
        "df[\"sched_dep_humidity\"] = df[\"id\"].map(sched_dep_humidity_dict)\n",
        "df[\"sched_dep_dew_point\"] = df[\"id\"].map(sched_dep_dew_point_dict)\n",
        "df[\"sched_dep_clouds\"] = df[\"id\"].map(sched_dep_clouds_dict)\n",
        "df[\"sched_dep_visibility\"] = df[\"id\"].map(sched_dep_visibility_dict)\n",
        "df[\"sched_dep_wind_deg\"] = df[\"id\"].map(sched_dep_wind_deg_dict)\n",
        "df[\"sched_dep_wind_speed\"] = df[\"id\"].map(sched_dep_wind_speed_dict)\n",
        "df[\"sched_dep_weather_main\"] = df[\"id\"].map(sched_dep_weather_main_dict)\n",
        "df[\"sched_dep_weather_main_desc\"] = df[\"id\"].map(sched_dep_weather_main_desc_dict)\n",
        "\n",
        "df[\"sched_arr_temp\"] = df[\"id\"].map(arr_dep_temperatures_dict)\n",
        "df[\"sched_arr_pressure\"] = df[\"id\"].map(arr_dep_pressure_dict)\n",
        "df[\"sched_arr_humidity\"] = df[\"id\"].map(arr_dep_humidity_dict)\n",
        "df[\"sched_arr_dew_point\"] = df[\"id\"].map(arr_dep_dew_point_dict)\n",
        "df[\"sched_arr_clouds\"] = df[\"id\"].map(arr_dep_clouds_dict)\n",
        "df[\"sched_arr_visibility\"] = df[\"id\"].map(arr_dep_visibility_dict)\n",
        "df[\"sched_arr_wind_deg\"] = df[\"id\"].map(arr_dep_wind_deg_dict)\n",
        "df[\"sched_arr_wind_speed\"] = df[\"id\"].map(arr_dep_wind_speed_dict)\n",
        "df[\"sched_arr_weather_main\"] = df[\"id\"].map(arr_dep_weather_main_dict)\n",
        "df[\"sched_arr_weather_main_desc\"] = df[\"id\"].map(arr_dep_weather_main_desc_dict)\n",
        "\n",
        "df.to_csv(\"flights_feature_engineering_third.csv\", index=False)\n",
        "print(\"\\nThe dataset has been saved as 'flights_feature_engineering_third'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The dataset has been saved as 'flights_feature_engineering_third'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCI00vOZ4ZV6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df = pd.read_csv('feature_engineering.csv')\n",
        "\n",
        "print(\"Columns in the dataset:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "df['totalDelay'] = df['total_delay'] - df['total_delay'].min()\n",
        "df['total_delay_log'] = np.log(df['totalDelay'] + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Columns in the dataset:\n",
        "\n",
        "\n",
        "['id', 'year', 'month', 'day', 'dep_time', 'sched_dep_time', 'dep_delay', 'arr_time', 'sched_arr_time', 'arr_delay', 'carrier', 'flight', 'tailnum', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute', 'time_hour_x', 'name', 'missing_count', 'flight_status', 'total_delay', 'date', 'is_holiday', 'season', 'dep_time_category', 'arr_time_category', 'temp', 'dewp', 'humid', 'wind_dir', 'wind_speed', 'wind_gust', 'precip', 'pressure', 'visib', 'time_hour_y', 'precip_category', 'visib_category', 'wind_speed_category', 'wind_gust_category', 'temp_category', 'pressure_category']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering\n",
        "\n",
        "This will put each date on a unit circle to help maintain the cyclic features. We also encode the numeric and categorical attributes to be trained by each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIbx5rRP4ZV6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Cyclical Encoding for day and month\n",
        "def convert_to_circle(column, max_value):\n",
        "    \"\"\"Convert a column to cyclical features using sine and cosine.\"\"\"\n",
        "    angle = 2 * np.pi * (column - 1) / max_value\n",
        "    x = np.cos(angle)  # x-coordinate on the unit circle\n",
        "    y = np.sin(angle)  # y-coordinate on the unit circle\n",
        "    return x, y\n",
        "\n",
        "# Time Hour to datetime\n",
        "df['time_hour_x'] = pd.to_datetime(df['time_hour_x'])\n",
        "\n",
        "df['hour'] = df['time_hour_x'].dt.hour\n",
        "df['day_of_week'] = df['time_hour_x'].dt.dayofweek\n",
        "df['month'] = df['time_hour_x'].dt.month\n",
        "\n",
        "# Apply cyclical encoding to day_of_week and month\n",
        "df['day_x'], df['day_y'] = convert_to_circle(df['day_of_week'], 7)\n",
        "df['month_x'], df['month_y'] = convert_to_circle(df['month'], 12)\n",
        "\n",
        "# Numerical attributes for modeling\n",
        "categorical_features =  ['carrier', 'origin', 'dest', 'flight_status', 'tailnum', 'is_holiday', 'season']\n",
        "\n",
        "# Numerical Attributes for modeling\n",
        "numerical_features = ['dep_time', 'arr_time', 'air_time', 'distance', 'hour', 'day_x', 'month_x', 'temp', 'wind_speed', 'wind_gust', 'precip', 'pressure', 'visib']\n",
        "\n",
        "# Create a ColumnTransformer to apply OneHotEncoder to categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "\n",
        "# X contains only the features we want, Y is total_delay\n",
        "X = df[categorical_features + numerical_features]\n",
        "y = df['total_delay_log']\n",
        "\n",
        "# Apply preprocessing\n",
        "X_preprocessed = preprocessor.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CJRogfI4ZV6"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-IFVJNh4ZV6"
      },
      "outputs": [],
      "source": [
        "offset = np.abs(np.min(y_train)) + 1\n",
        "y_train_adj = y_train + offset\n",
        "y_test_adj = y_test + offset\n",
        "\n",
        "# Train XGBoost Poisson\n",
        "xgb_poisson = XGBRegressor(\n",
        "    objective='count:poisson',\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "xgb_poisson.fit(X_train, y_train_adj)\n",
        "\n",
        "\n",
        "y_pred_poisson = xgb_poisson.predict(X_test) - offset\n",
        "\n",
        "# Evaluate\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred_poisson))\n",
        "print(f\"RMSE: {rmse:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "XGBoost RMSE: 28.438963300620795"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV0Ht2QC4ZV6"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Cross Vlidation with n-folds\n",
        "def regression_cv_xgb(model, X, y, n_folds=5):\n",
        "    scores = -cross_val_score(model, X, y,\n",
        "                            cv=n_folds,\n",
        "                            scoring='neg_root_mean_squared_error',\n",
        "                            n_jobs=-1)\n",
        "    return scores\n",
        "\n",
        "# Grid search\n",
        "def grid_search_xgb(params_grid, X, y, n_folds=5):\n",
        "\n",
        "    best_score = float('inf')\n",
        "    best_std = float('inf')\n",
        "    best_params = None\n",
        "    results = []\n",
        "\n",
        "    for values in product(*params_grid.values()):\n",
        "        params = dict(zip(params_grid.keys(), values))\n",
        "        params.update({\n",
        "            'objective': 'count:poisson',\n",
        "            'eval_metric': 'poisson-nloglik',\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        })\n",
        "\n",
        "        print(f\"Testing params: {params}\")\n",
        "\n",
        "        model = XGBRegressor(**params)\n",
        "        scores = regression_cv_xgb(model, X, y, n_folds=n_folds)\n",
        "\n",
        "        mean_rmse = np.mean(scores)\n",
        "        std_rmse = np.std(scores)\n",
        "\n",
        "        results.append({\n",
        "            \"params\": params,\n",
        "            \"mean_rmse\": mean_rmse,\n",
        "            \"std_rmse\": std_rmse,\n",
        "            \"scores\": scores\n",
        "        })\n",
        "\n",
        "        print(f\" Mean RMSE: {mean_rmse:.4f} (±{std_rmse:.4f})\\n\")\n",
        "\n",
        "        if mean_rmse < best_score:\n",
        "            best_score = mean_rmse\n",
        "            best_params = params\n",
        "            best_std = std_rmse\n",
        "            print(\"=======NEW BEST======\")\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(f\"\\n Best Params: {best_params}\")\n",
        "    print(f\" Best RMSE mean: {best_score:.4f}\")\n",
        "    print(f\" Best RMSE std: {best_std:.4f}\")\n",
        "\n",
        "    return best_params, results_df\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# grid search\n",
        "best_params, results_df = grid_search_xgb(param_grid, X_train, y_train, n_folds=5)\n",
        "\n",
        "# results\n",
        "results_df.to_csv(\"xgb_grid_search_poisson_results.csv\", index=False)\n",
        "\n",
        "# Model training with best parameters\n",
        "final_model = XGBRegressor(**best_params)\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = final_model.predict(X_test)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"\\n Final Test RMSE: {test_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing params: {'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 100, 'gamma': 0, 'objective': 'count:poisson', 'eval_metric': 'poisson-nloglik', 'random_state': 42, 'n_jobs': -1}\n",
        " Mean RMSE: 0.4172 (±0.0022)\n",
        "\n",
        "=======NEW BEST======\n",
        "Testing params: {'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 100, 'gamma': 0.1, 'objective': 'count:poisson', 'eval_metric': 'poisson-nloglik', 'random_state': 42, 'n_jobs': -1}\n",
        " Mean RMSE: 0.4172 (±0.0022)\n",
        "\n",
        "Testing params: {'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 100, 'gamma': 0.2, 'objective': 'count:poisson', 'eval_metric': 'poisson-nloglik', 'random_state': 42, 'n_jobs': -1}\n",
        " Mean RMSE: 0.4172 (±0.0022)\n",
        "\n",
        "Testing params: {'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 200, 'gamma': 0, 'objective': 'count:poisson', 'eval_metric': 'poisson-nloglik', 'random_state': 42, 'n_jobs': -1}\n",
        " Mean RMSE: 0.3978 (±0.0022)\n",
        "\n",
        "=======NEW BEST======\n",
        "Testing params: {'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 200, 'gamma': 0.1, 'objective': 'count:poisson', 'eval_metric': 'poisson-nloglik', 'random_state': 42, 'n_jobs': -1}\n",
        " Mean RMSE: 0.3978 (±0.0022)\n",
        "\n",
        "Testing params: {'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 200, 'gamma': 0.2, 'objective': 'count:poisson', 'eval_metric': 'poisson-nloglik', 'random_state': 42, 'n_jobs': -1}\n",
        " Mean RMSE: 0.3978 (±0.0022)\n",
        "\n",
        "Testing params: {'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 300, 'gamma': 0, 'objective': 'count:poisson', 'eval_metric': 'poisson-nloglik', 'random_state': 42, 'n_jobs': -1}\n",
        " Mean RMSE: 0.3832 (±0.0019)\n",
        "\n",
        "=======NEW BEST======\n",
        "Testing params: {'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 300, 'gamma': 0.1, 'objective': 'count:poisson', 'eval_metric': 'poisson-nloglik', 'random_state': 42, 'n_jobs': -1}\n",
        "...\n",
        " Best RMSE mean: 0.1868\n",
        " Best RMSE std: 0.0011\n",
        "\n",
        " Final Test RMSE: 0.1862"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Here we train XGBoost poisson with the best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjeWkO2E4ZV6"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "# XGBoost parameters\n",
        "params = {\n",
        "    'objective': 'count:poisson',\n",
        "    'eval_metric': 'poisson-nloglik',\n",
        "    'learning_rate': 0.01,\n",
        "    'max_depth': 3,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'n_estimators': 300,\n",
        "    'gamma': 0.01,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "num_rounds = 1000\n",
        "model1 = xgb.train(params, dtrain, num_rounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAHalUZT4ZV6"
      },
      "outputs": [],
      "source": [
        "y_pred1 = model1.predict(dtest)\n",
        "print(y_pred1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[4.6325793 5.3424296 6.1335955 ... 4.5085764 4.899206  4.6391454]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These values are just the predicted log transformation total delay. This will not provide anything meaningful as Poisson assumes these are count values of number of delays. Therefore we cannot calculate the probability."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
